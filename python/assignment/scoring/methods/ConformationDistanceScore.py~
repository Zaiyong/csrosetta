#!/usr/bin/env python2.7
##-*- mode:python;tab-width:2;indent-tabs-mode:t;show-trailing-whitespace:t;rm-trailing-spaces:t;python-indent:2 -*-'

#score method based on conformation(PDB structures) distance distribution

import math
import library
from basic import options
from basic import Tracer
from assignment.scoring import score_definitions
from conformation import Conformation
from ScoreMethod import ScoreMethod,ScoreOptions,ScoreItem
from assignment import PeakList, Peak

SCORE_TYPE='pdb'
parser = options.ModuleArgumentParser("%s-score"%SCORE_TYPE,
																			prefix=SCORE_TYPE,
																			description='options for scoring against distance of structures',
																			add_help=False )
parser.add_argument("-file",help="pdb structure file ", default=None )
parser.add_argument("-norm", help='expect x-percent of frag_dist below noesy_cut_off [default 0.3]', type=float, default=0.3 )

tr=Tracer('scoring.%s'%SCORE_TYPE)

class ConformationDistOptions(ScoreOptions):
	def __init__(self,norm=0):
		if not norm:
			args=parser.parse_args()
			norm=args.norm
	  self.prob_norm=1-norm

#######################
##
## class FragDistanceScore
##
## reads a dist-table generated with FragsToAtomDist.default.linuxgccrelease
## NOESY assignments to atom-pairs with distances small according to fragments
## get favorable score
## we evaluate the integral of distance distribution above the noesy cutoff
## if almost all distances are above the noesy cutoff it is unlikely that this assignment is correct
## however: what about long-range assignments ?
## here we have no data and return a neutral 0
##
## TODO: should maybe have some smooth weighting scheme that takes into account that information on 1-9 distances is less accurate
## than information on 1-1 and 1-2 distances
class ConformationDistanceScore(ScoreMethod):
	pdb_lib=None
	#end DistanceMatcher

	def __init__(self,options=None):
		if not options:
			options=ConformationDistOptions()
		#obtain raw data, either from cmd-line or as list of stuff
		super(ConformationDistanceScore,self).__init__(SCORE_TYPE,options)
		if not ConformationDistanceScore.pdb_lib:#read pdb files if necessary
			ConformationDistanceScore.load_pdb()

	@staticmethod
	def load_pdb():
		import time
		start = time.clock()
		args=parser.parse_args()
		if not args.file:
			raise library.MissingInput('A pdb file is required. Load with -pdb_file')
		tr.Info('reading conformation data from %s...'%args.file)
		ConformationDistanceScore.pdb_lib=Conformation.from_pdb_file(args.file)
		elapsed = (time.clock() - start)
		tr.Info('read pdb file took %5.1f seconds.'%elapsed)

	def score(self,assignments):
		super(ConformationDistanceScore,self).score(assignments)
		#initialize q0 from norm
		q0=abs(math.log(1.0-self.options.prob_norm))
		total_score=0
		for assignment in assignments:
			try:
				total_score+=assignments.scores.assignments[assignment][self.name].score
			except KeyError:
				score=self.score_assignment( assignment, q0 )
				assignments.scores.assignments.setdefault(assignment,{})[self.name]=ScoreItem(score)
				total_score+=score
		return total_score

	def score_assignment( self, assignment, q0=None ):
		atom_pairs=assignment.distance_atoms()
		score=0
		if not q0: q0=abs(math.log(1.0-self.options.prob_norm))
		for pair in atom_pairs:
			if abs(pair[0].resid-pair[1].resid)<9:
				score+=self._score_atom_pair(q0,assignment.rule.distance_cutoff,pair[0],pair[1])
			#		print 'frag_score=%8.3f for %s'%(score,assignment)
		return score

	def _score_atom_pair(self,q0,noesy_dist_cutoff,atom1,atom2):
		if atom1==atom2:
			return 1.0

		distribution=self.pdb_lib.distribution_by_atom_pair(atom1,atom2)
		#compute the excluded probability weight
		bad_prob=distribution.integral(noesy_dist_cutoff,distribution.high())
		try:
			q=math.log(1-bad_prob)
		except ValueError:
			q=score_definitions.VERY_LOW_LOG
#		print 'p(no_contact)=%5.2f score=%7.2f'%(bad_prob,1.0+q/self.q0)
		return 1.0+q/q0

	def notify_add(self, state, new_assignment):
		score=self.score_assignment( new_assignment )
		state.scores.assignments.setdefault(new_assignment,{})[self.name]=ScoreItem(score)

	def notify_remove(self, scores, old_assignment):
		#cannot nuke the whole assignment entry, that could mess-up other ScoreMethods, e.g., the SymmetryScore
		scores.assignments[old_assignment].pop(self.name)

	def copy_cache( self, new_cache, old_cache ):
		new_cache.simple_cache_copy_from( old_cache, self.name, ['assignments'] )

	#this fills the list of expected peaks in the PeakCollection
	def fill_expected_peaks(self, peak_collection, molecule):
		from assignment.rules import ScoreDistanceMatcher
		expected_peaks=PeakList('expected')
		peak_collection.expected=expected_peaks
		for name,peak_list in peak_collection.experiments.iteritems():
			tr.Info('compute expected peaks for peak list %s...'%name)
			try:
				exp_peak=peak_list[0]
				peak_dim=exp_peak.dim
				rule=exp_peak.rule
				peak=Peak((None,)*peak_dim,rule,id=SCORE_TYPE,pl_name='expected_'+name)
				distance_matcher=ScoreDistanceMatcher(self, abs(math.log(1.0-self.options.prob_norm)), 0 )
				expected_peaks+=[ m for m in peak.matches( molecule, random_samples=None, frequency_matcher=None, distance_matcher=distance_matcher ) ]
			except IndexError:
				continue
#			break
#		tr.Warning('[WARNING] for testing early bail-out from peak-generation')
		tr.Info('done!')
import ScoreMethodManager
ScoreMethodManager.register(SCORE_TYPE,ConformationDistanceScore)


# import unittest

# #### unit testing
# class ConformationDistanceScoreDistributionTestCase(unittest.TestCase):
# 	def setUp(self):
# 		import basic
# 		self._data_path = basic.get_unittest_data()
# 		self._pdb=self._data_path+'/2lxt.pdb.chainA'
# 		self._conformation_distance_distribution_score_method=FragDistanceScoreDistribution(lines)
# 	def test_high_and_low(self):
# 		real_high=[3.609,6.976,9.754,9.847, 9.715, 10.17,8.204,10.71,8.808,7.105]
# 		real_low =[2.171,1.567,3.207,5.037,5.416,4.859,1.155, 3.195,6.272, 5.386]
# 		high=[]
# 		low=[]
# 		for atom1,atom2 in zip(self._atom_list1,self._atom_list2):
# 			high.append(self._frag_distance_distribution_score_method.distribution_library().by_atom_pair(atom1,atom2).high())
# 			low.append(self._frag_distance_distribution_score_method.distribution_library().by_atom_pair(atom1,atom2).low())
# 		for h,rh,a1,a2 in zip(high,real_high,self._atom_list1,self._atom_list2):
# 			self.assertAlmostEqual(h,rh,3,'highest distance of atom %s and %s should be %5.3f but we get %5.3f (which is wrong)'%(a1,a2,rh,h))
# 		for l,rl,a1,a2 in zip(low,real_low,self._atom_list1,self._atom_list2):
# 			self.assertAlmostEqual(l,rl,3,'lowest distance of atom %s and %s should be %5.3f but we get %5.3f (which is wrong)'%(a1,a2,rl,l))
# 	def test_seq(self):
# 		real_seq='MNLTVNGKPSTVDGAESLNVTELLSALKVAQAEYVTVELNGEVLEREAFDATTVKDGDAVEFLYFM'
# 		seq=self._frag_distance_distribution_score_method.distribution_library().seq()
# 		self.assertEqual(seq,real_seq,'The sequence of this frag2dist file should be %s \n but we get %s'%(real_seq,seq))

# 	def test_hist_and_edges(self):
# 		real_hist=[0.0450,  0.0200,  0.1550,  0.1700,  0.1500,  0.1900,  0.0700,  0.0450,  0.0350,  0.0200,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0050,  0.0000,  0.0000,  0.0000,  0.0950]
# 		real_edges=[2.1710,  2.2429,  2.3148,  2.3867,  2.4586,  2.5305,  2.6024,  2.6743,  2.7462,  2.8181,  2.8900,  2.9619,  3.0338,  3.1057,  3.1776,  3.2495,  3.3214,  3.3933,  3.4652,  3.5371,  3.6090]
# 		hist=self._frag_distance_distribution_score_method.distribution_library().by_atom_pair(Atom('H',1),Atom('HB2',1)).hist()
# 		edges=self._frag_distance_distribution_score_method.distribution_library().by_atom_pair(Atom('H',1),Atom('HB2',1)).bin_edges()
# 		for h,rh in zip(hist,real_hist):
# 			self.assertAlmostEqual(h,rh,3,'one distance hist of should be %5.3f but we get %5.3f (which is wrong)'%(rh,h))
# 		for e,re in zip(edges,real_edges):
# 			self.assertAlmostEqual(e,re,3,'one distance hist edge of should be %5.3f but we get %5.3f (which is wrong)'%(re,e))

# def FragDistanceScoreDistributionTestSuite():
# 	suite = unittest.TestLoader().loadTestsFromTestCase(FragDistanceScoreDistributionTestCase)
# 	return suite


#if __name__ == '__main__':
#	unittest.main()

